# Default values for omnia.
# This is a YAML-formatted file.

# -- Number of operator replicas
replicaCount: 1

image:
  # -- Image repository
  repository: ghcr.io/altairalabs/omnia
  # -- Image pull policy
  pullPolicy: IfNotPresent
  # -- Image tag (defaults to Chart appVersion)
  tag: ""

# -- Image pull secrets
imagePullSecrets: []

# -- Override the name of the chart
nameOverride: ""

# -- Override the full name of the chart
fullnameOverride: ""

serviceAccount:
  # -- Create a service account
  create: true
  # -- Annotations to add to the service account
  annotations: {}
  # -- The name of the service account (generated if not set)
  name: ""

# -- Pod annotations
podAnnotations: {}

# -- Pod security context
podSecurityContext:
  runAsNonRoot: true
  seccompProfile:
    type: RuntimeDefault

# -- Container security context
securityContext:
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL

# -- Resource limits and requests
resources:
  limits:
    cpu: 500m
    memory: 256Mi
  requests:
    cpu: 10m
    memory: 128Mi

# -- Node selector
nodeSelector: {}

# -- Tolerations
tolerations: []

# -- Affinity rules
affinity: {}

# Leader election configuration
leaderElection:
  # -- Enable leader election for HA
  enabled: true

# Health probes configuration
probes:
  # -- Port for health probes
  port: 8081
  liveness:
    # -- Initial delay for liveness probe
    initialDelaySeconds: 15
    # -- Period for liveness probe
    periodSeconds: 20
  readiness:
    # -- Initial delay for readiness probe
    initialDelaySeconds: 5
    # -- Period for readiness probe
    periodSeconds: 10

# Metrics configuration
metrics:
  # -- Enable metrics endpoint
  enabled: false
  # -- Port for metrics
  port: 8443
  # -- Enable secure metrics (HTTPS)
  secure: true

# Webhook configuration
webhook:
  # -- Enable webhooks
  enabled: false
  # -- Port for webhooks
  port: 9443

# REST API configuration (for dashboard access)
api:
  # -- Enable REST API server
  enabled: true
  # -- Port for REST API
  port: 8082

# RBAC configuration
rbac:
  # -- Create RBAC resources
  create: true

# CRD configuration
crds:
  # -- Install CRDs with the chart
  install: true

# ============================================================================
# Dashboard Configuration
# Web UI for managing Omnia agents and resources
# ============================================================================

dashboard:
  # -- Enable dashboard deployment
  enabled: true

  image:
    # -- Dashboard image repository
    repository: ghcr.io/altairalabs/omnia-dashboard
    # -- Dashboard image pull policy
    pullPolicy: IfNotPresent
    # -- Dashboard image tag (defaults to Chart appVersion)
    tag: ""

  # -- Number of dashboard replicas
  replicaCount: 1

  # -- Dashboard service configuration
  service:
    # -- Service type (ClusterIP, NodePort, LoadBalancer)
    type: ClusterIP
    # -- Service port
    port: 3000

  # -- Dashboard ingress configuration
  ingress:
    # -- Enable ingress
    enabled: false
    # -- Ingress class name
    className: ""
    # -- Ingress annotations
    annotations: {}
    # -- Ingress host
    host: dashboard.example.com
    # -- TLS configuration
    tls: []
    #  - secretName: dashboard-tls
    #    hosts:
    #      - dashboard.example.com

  # -- Dashboard resource limits and requests
  resources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi

  # -- Dashboard pod annotations
  podAnnotations: {}

  # -- Dashboard node selector
  nodeSelector: {}

  # -- Dashboard tolerations
  tolerations: []

  # -- Dashboard affinity rules
  affinity: {}

  # -- Authentication configuration
  auth:
    # -- Authentication mode: anonymous, proxy, oauth, or builtin
    mode: anonymous
    # -- Session secret (required for non-anonymous modes)
    # Generate with: openssl rand -base64 32
    sessionSecret: ""
    # -- Use existing secret for session secret
    # Secret must contain OMNIA_SESSION_SECRET key
    existingSessionSecret: ""
    # -- Session cookie name
    cookieName: omnia_session
    # -- Session TTL in seconds (default: 24 hours)
    ttl: 86400
    # -- Default role for anonymous users
    anonymousRole: viewer
    # -- Role mapping from groups
    roleMapping:
      # -- Groups that get admin role
      adminGroups: []
      # -- Groups that get editor role
      editorGroups: []

  # -- Proxy authentication settings (when auth.mode=proxy)
  proxy:
    # -- Header containing username
    headerUser: X-Forwarded-User
    # -- Header containing email
    headerEmail: X-Forwarded-Email
    # -- Header containing groups
    headerGroups: X-Forwarded-Groups
    # -- Header containing display name
    headerDisplayName: X-Forwarded-Preferred-Username
    # -- Auto-create users on first login
    autoSignup: true

  # -- OAuth settings (when auth.mode=oauth)
  oauth:
    # -- OAuth provider: generic, google, github, azure, okta
    provider: generic
    # -- OAuth client ID
    clientId: ""
    # -- OAuth client secret
    clientSecret: ""
    # -- Use existing secret for OAuth credentials
    # Secret must contain OMNIA_OAUTH_CLIENT_ID and OMNIA_OAUTH_CLIENT_SECRET keys
    existingSecret: ""
    # -- OIDC issuer URL (required for generic provider)
    issuerUrl: ""
    # -- OAuth scopes (comma-separated)
    scopes: "openid,profile,email"
    # -- Azure AD tenant ID (for azure provider)
    azureTenantId: ""
    # -- Okta domain (for okta provider)
    oktaDomain: ""
    # -- Claim mapping
    claims:
      username: preferred_username
      email: email
      displayName: name
      groups: groups

  # -- Builtin authentication settings (when auth.mode=builtin)
  builtin:
    # -- Storage backend: sqlite or postgresql
    storeType: sqlite
    # -- SQLite database path (for sqlite store)
    sqlitePath: /data/omnia-users.db
    # -- PostgreSQL connection URL (for postgresql store)
    # Use existingSecret for production
    postgresUrl: ""
    # -- Use existing secret for PostgreSQL URL
    # Secret must contain OMNIA_BUILTIN_POSTGRES_URL key
    existingPostgresSecret: ""
    # -- Allow public signup
    allowSignup: false
    # -- Require email verification
    verifyEmail: false
    # -- Minimum password length
    minPasswordLength: 8
    # -- Max failed login attempts before lockout
    maxFailedAttempts: 5
    # -- Lockout duration in seconds
    lockoutDuration: 900
    # -- Initial admin user (created on first run)
    admin:
      # -- Admin username
      username: admin
      # -- Admin email
      email: admin@example.com
      # -- Admin password (use existingSecret for production)
      password: ""
      # -- Use existing secret for admin password
      # Secret must contain OMNIA_BUILTIN_ADMIN_PASSWORD key
      existingSecret: ""

  # -- API keys configuration
  apiKeys:
    # -- Enable API key authentication
    enabled: true
    # -- Maximum keys per user
    maxPerUser: 10
    # -- Default expiration in days (0 = never)
    defaultExpiration: 90

  # -- Operator API URL (for dashboard to communicate with operator)
  # Defaults to in-cluster service URL
  operatorApiUrl: ""

  # -- Enable read-only mode (disables mutations)
  readOnlyMode: false

  # -- Custom read-only message
  readOnlyMessage: ""

  # -- Grafana integration (direct browser access)
  grafana:
    # -- Grafana URL for embedded metrics (must be browser-accessible)
    # For local dev: http://localhost:3001
    # For production: use ingress URL
    url: ""
    # -- Grafana subpath (must match Grafana's serve_from_sub_path setting)
    path: /grafana/
    # -- Grafana organization ID
    orgId: 1

  # -- Prometheus integration (for cost metrics)
  prometheus:
    # -- Prometheus URL for cost queries
    # Defaults to in-cluster Prometheus service when prometheus.enabled=true
    url: ""

  # -- WebSocket proxy configuration (for agent console connections)
  wsProxy:
    # -- WebSocket proxy URL for browser connections
    # For local dev: ws://localhost:3002
    # For production with gateway: leave empty (uses gateway routing)
    url: ""

  # -- Persistent volume for builtin auth SQLite database
  persistence:
    # -- Enable persistence (required for builtin auth with SQLite)
    enabled: false
    # -- Storage class
    storageClass: ""
    # -- Access modes
    accessModes:
      - ReadWriteOnce
    # -- Volume size
    size: 1Gi
    # -- Existing PVC to use
    existingClaim: ""

# ============================================================================
# Arena Fleet Configuration
# Distributed testing framework for evaluating PromptKit bundles
# ============================================================================

arena:
  # -- Enable Arena Fleet controllers
  enabled: true

  # Worker configuration for ArenaJob execution
  worker:
    image:
      # -- Worker image repository
      repository: ghcr.io/altairalabs/arena-worker
      # -- Worker image tag (defaults to Chart appVersion)
      tag: ""
      # -- Worker image pull policy
      pullPolicy: IfNotPresent

    # -- Default resource limits for worker pods
    resources:
      limits:
        cpu: 1000m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 128Mi

  # Source fetcher configuration
  source:
    # -- Interval between source checks (default: 5m)
    defaultInterval: 5m
    # -- Timeout for fetching sources
    fetchTimeout: 2m

  # Result storage configuration (future use)
  storage:
    # -- Storage type: s3, pvc, or memory (memory is for testing only)
    type: memory
    # S3 configuration (when type: s3)
    s3:
      # -- S3 bucket for results
      bucket: ""
      # -- S3 region
      region: ""
      # -- S3 endpoint (for S3-compatible storage)
      endpoint: ""
      # -- Secret containing AWS credentials
      secretRef: ""
    # PVC configuration (when type: pvc)
    pvc:
      # -- Existing PVC for results
      claimName: ""
      # -- Storage class for dynamic provisioning
      storageClass: ""
      # -- Volume size
      size: 10Gi

# Facade container configuration (used by AgentRuntime)
facade:
  image:
    # -- Facade image repository
    repository: ghcr.io/altairalabs/omnia-facade
    # -- Facade image tag (defaults to Chart appVersion)
    tag: ""
    # -- Facade image pull policy
    pullPolicy: IfNotPresent

# Framework container configuration (used by AgentRuntime)
# This aligns with the CRD's spec.framework field
framework:
  image:
    # -- Framework (runtime) image repository
    repository: ghcr.io/altairalabs/omnia-runtime
    # -- Framework image tag (defaults to Chart appVersion)
    tag: ""
    # -- Framework image pull policy
    pullPolicy: IfNotPresent

# Observability configuration (for Omnia-specific templates)
observability:
  # -- Enable Omnia dashboards/datasources
  enabled: true

# ============================================================================
# Tracing Configuration
# Distributed tracing for agent runtime containers
# ============================================================================

tracing:
  # -- Enable distributed tracing for agent runtime containers
  enabled: false
  # -- OTLP gRPC endpoint for traces
  # When tempo.enabled=true, defaults to the in-cluster Tempo service
  # Example: omnia-tempo.omnia-system.svc.cluster.local:4317
  endpoint: ""

# ============================================================================
# Subchart configurations (optional observability stack)
# Set enabled: true to deploy each component
# ============================================================================

# Prometheus configuration (metrics)
# See: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus
prometheus:
  # -- Enable Prometheus
  enabled: false
  server:
    # -- Disable persistent storage for dev/test
    persistentVolume:
      enabled: false
    # Configure Prometheus to serve from sub-path
    prefixURL: /prometheus
    baseURL: /prometheus
    # Enable remote write receiver for Tempo metrics-generator
    extraFlags:
      - web.enable-remote-write-receiver
  # -- Disable alertmanager
  alertmanager:
    enabled: false
  # -- Disable pushgateway
  prometheus-pushgateway:
    enabled: false
  # -- Disable node-exporter
  prometheus-node-exporter:
    enabled: false
  # -- Disable kube-state-metrics (usually already present)
  kube-state-metrics:
    enabled: false
  # -- Extra scrape configs for Omnia agent metrics
  extraScrapeConfigs: |
    # Scrape Omnia facade containers (WebSocket connections, media uploads)
    # Uses prometheus.io annotations which point to facade port 8080
    - job_name: 'omnia-agents-facade'
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        # Only scrape pods with omnia agent label
        - source_labels: [__meta_kubernetes_pod_label_omnia_altairalabs_ai_component]
          action: keep
          regex: agent
        # Use prometheus.io annotations (works for both Istio and non-Istio pods)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        # Add useful labels
        - source_labels: [__meta_kubernetes_namespace]
          target_label: namespace
        - source_labels: [__meta_kubernetes_pod_name]
          target_label: pod
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_instance]
          target_label: agent
        - target_label: container
          replacement: facade
    # Scrape Omnia runtime containers (LLM metrics: tokens, cost, latency)
    # Runtime exposes metrics on port 9001 from PromptKit EventBus
    - job_name: 'omnia-agents-runtime'
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        # Only scrape pods with omnia agent label
        - source_labels: [__meta_kubernetes_pod_label_omnia_altairalabs_ai_component]
          action: keep
          regex: agent
        # Target runtime container's health port (9001) for LLM metrics
        - source_labels: [__address__]
          action: replace
          regex: ([^:]+)(?::\d+)?
          replacement: $1:9001
          target_label: __address__
        # Add useful labels
        - source_labels: [__meta_kubernetes_namespace]
          target_label: namespace
        - source_labels: [__meta_kubernetes_pod_name]
          target_label: pod
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_instance]
          target_label: agent
        - target_label: container
          replacement: runtime

# Grafana configuration (dashboards)
# See: https://github.com/grafana/helm-charts/tree/main/charts/grafana
grafana:
  # -- Enable Grafana
  enabled: false
  # -- Admin password (change in production!)
  adminPassword: admin
  # Configure Grafana to serve from sub-path
  grafana.ini:
    server:
      root_url: "%(protocol)s://%(domain)s:%(http_port)s/grafana/"
      serve_from_sub_path: true
    # Enable anonymous access for iframe embedding
    auth.anonymous:
      enabled: true
      org_role: Viewer
    # Allow embedding in iframes
    security:
      allow_embedding: true
  # Sidecar configuration for auto-loading dashboards/datasources
  sidecar:
    dashboards:
      # -- Enable dashboard sidecar
      enabled: true
      # -- Label to identify dashboard ConfigMaps
      label: grafana_dashboard
      # -- Search all namespaces for dashboards
      searchNamespace: ALL
    datasources:
      # -- Enable datasource sidecar
      enabled: true
      # -- Label to identify datasource ConfigMaps
      label: grafana_datasource
      # -- Search all namespaces for datasources
      searchNamespace: ALL
  # -- Grafana service type
  service:
    type: ClusterIP

# Loki configuration (logs)
# See: https://github.com/grafana/helm-charts/tree/main/charts/loki
loki:
  # -- Enable Loki
  enabled: false
  # -- Deploy in single binary mode
  deploymentMode: SingleBinary
  loki:
    # -- Disable auth for simplicity
    auth_enabled: false
    # -- Use test schema for quick deployment
    useTestSchema: true
    # -- Use filesystem storage
    storage:
      type: filesystem
    # -- Common config for single-binary mode
    commonConfig:
      replication_factor: 1
    # -- Enable log volume for Grafana Explore
    limits_config:
      volume_enabled: true
  singleBinary:
    # -- Single replica for dev/test
    replicas: 1
    persistence:
      # -- Enable persistence for dev/test
      enabled: true
      size: 10Gi
  # -- Disable distributed components
  backend:
    replicas: 0
  read:
    replicas: 0
  write:
    replicas: 0
  # -- Disable gateway
  gateway:
    enabled: false
  # -- Disable caches for simpler deployment
  chunksCache:
    enabled: false
  resultsCache:
    enabled: false
  # -- Disable ruler to avoid "no space left on device" errors on Docker Desktop
  # The ruler tries to mkdir /var/loki/rules which fails with local-path provisioner
  # See: https://github.com/grafana/helm-charts/issues/577
  ruler:
    enabled: false

# Alloy configuration (unified telemetry collector)
# See: https://github.com/grafana/helm-charts/tree/main/charts/alloy
# Alloy replaces Promtail and can collect logs, metrics, and traces
alloy:
  # -- Enable Alloy telemetry collector
  enabled: false
  alloy:
    # Alloy configuration using River syntax
    configMap:
      content: |
        // Discover Kubernetes pods
        discovery.kubernetes "pods" {
          role = "pod"
        }

        // Relabel discovered pods for log collection
        discovery.relabel "pods" {
          targets = discovery.kubernetes.pods.targets

          // Keep only running pods
          rule {
            source_labels = ["__meta_kubernetes_pod_phase"]
            regex         = "Pending|Succeeded|Failed|Unknown"
            action        = "drop"
          }

          // Set namespace label
          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            target_label  = "namespace"
          }

          // Set pod label
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            target_label  = "pod"
          }

          // Set container label
          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            target_label  = "container"
          }

          // Set app label from pod label
          rule {
            source_labels = ["__meta_kubernetes_pod_label_app"]
            target_label  = "app"
          }

          // Set app.kubernetes.io/name label
          rule {
            source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
            target_label  = "app_name"
          }

          // Build log file path
          rule {
            source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
            separator     = "/"
            target_label  = "__path__"
            replacement   = "/var/log/pods/*$1/*.log"
          }
        }

        // Collect logs from discovered pods
        loki.source.kubernetes "pods" {
          targets    = discovery.relabel.pods.output
          forward_to = [loki.process.pods.receiver]
        }

        // Process logs (parse JSON, extract level, filter noise)
        loki.process "pods" {
          forward_to = [loki.write.default.receiver]

          // Parse CRI format
          stage.cri {}

          // Drop noisy health check and metrics logs
          stage.drop {
            expression  = ".*(GET|POST) /(health|healthz|ready|readyz|metrics|livez).*"
            drop_counter_reason = "health_metrics_filter"
          }

          // Try to parse JSON logs and extract timestamp
          stage.json {
            expressions = {
              level     = "level",
              msg       = "msg",
              caller    = "caller",
              timestamp = "time",
              ts        = "ts",
            }
          }

          // Extract timestamp from JSON 'time' field (common in structured logs)
          stage.timestamp {
            source = "timestamp"
            format = "RFC3339"
            fallback_formats = ["RFC3339Nano", "Unix", "UnixMs", "UnixUs", "UnixNs"]
          }

          // Fallback: try 'ts' field (used by zap and other loggers)
          stage.timestamp {
            source = "ts"
            format = "RFC3339"
            fallback_formats = ["RFC3339Nano", "Unix", "UnixMs", "UnixUs", "UnixNs"]
          }

          // Add level as label if extracted
          stage.labels {
            values = {
              level = "",
            }
          }

          // Store extracted fields as structured metadata (queryable but not indexed)
          // Requires Loki 3.0+ with structured metadata enabled
          stage.structured_metadata {
            values = {
              msg    = "",
              caller = "",
            }
          }

          // Output a clean log line (just the message if available)
          stage.output {
            source = "msg"
          }
        }

        // Write logs to Loki
        loki.write "default" {
          endpoint {
            url = "http://omnia-loki:3100/loki/api/v1/push"
          }
        }

# Tempo configuration (traces)
# See: https://github.com/grafana/helm-charts/tree/main/charts/tempo
tempo:
  # -- Enable Tempo
  enabled: false
  tempo:
    # -- Disable telemetry reporting
    reportingEnabled: false
    # Enable metrics generator for service graphs
    metricsGenerator:
      enabled: true
      remoteWriteUrl: "http://omnia-prometheus-server:80/prometheus/api/v1/write"
  # Disable Jaeger query UI (we use Grafana instead)
  tempoQuery:
    enabled: false
  # -- Enable persistence (required for Tempo storage)
  persistence:
    enabled: true
    size: 5Gi

# ============================================================================
# Istio Service Mesh Integration
# Istio should be installed separately before deploying Omnia
# See: https://istio.io/latest/docs/setup/install/helm/
# ============================================================================

istio:
  # -- Enable Istio integration (Telemetry resources for tracing/logging)
  # Requires Istio to be installed separately in istio-system namespace
  enabled: false
  # -- Tempo service for trace export (adjust if using different release name)
  tempoService: omnia-tempo.omnia-system.svc.cluster.local
  # -- Tempo OTLP port
  tempoPort: 4317

# ============================================================================
# Gateway API Configuration
# Kubernetes Gateway API is required for agent ingress
# ============================================================================

# External gateway for agent traffic (WebSocket connections)
gateway:
  # -- Enable external Gateway for agent ingress (requires Istio or Gateway controller)
  enabled: false
  # -- Gateway name suffix
  name: agents
  # -- Gateway class name (istio when using Istio, or external controller)
  className: istio
  # -- Gateway listener configuration
  listeners:
    # -- HTTP listener for agent WebSocket connections
    http:
      port: 80
      protocol: HTTP
    # -- HTTPS listener (requires TLS secret)
    https:
      enabled: false
      port: 443
      protocol: HTTPS
      # -- TLS secret name (must exist in the namespace)
      tlsSecretName: ""

# Internal gateway for observability tools (Grafana, Prometheus, etc.)
internalGateway:
  # -- Enable internal Gateway for observability tools (requires Istio or Gateway controller)
  enabled: false
  # -- Gateway name suffix
  name: internal
  # -- Gateway class name
  className: istio
  # -- Gateway listener port
  port: 8080
  # -- Expose Grafana (main observability UI - includes Prometheus, Loki, Tempo datasources)
  grafana:
    enabled: true
    path: /grafana
  # -- Expose Prometheus UI (optional - metrics also available in Grafana)
  prometheus:
    enabled: true
    path: /prometheus

# ============================================================================
# Authentication Configuration
# JWT-based authentication using Istio RequestAuthentication
# ============================================================================

authentication:
  # -- Enable JWT authentication for agent endpoints
  enabled: false

  # -- OIDC/JWT provider configuration
  jwt:
    # -- JWT issuer URL (e.g., https://auth.example.com, https://accounts.google.com)
    issuer: ""
    # -- JWKS URI for validating JWT signatures
    # If empty, defaults to {issuer}/.well-known/jwks.json
    jwksUri: ""
    # -- JWT audiences to accept (optional, validates 'aud' claim)
    audiences: []
    # -- Forward the original token to the upstream service
    forwardOriginalToken: true
    # -- Headers to output payload claims (optional)
    # Example: outputClaimToHeaders:
    #   - header: x-user-id
    #     claim: sub
    outputClaimToHeaders: []

  # -- Authorization rules (optional, defaults to requiring valid JWT)
  authorization:
    # -- Require specific claims in the JWT
    # Example: requiredClaims:
    #   - claim: "scope"
    #     values: ["agents:access"]
    requiredClaims: []
    # -- Allow unauthenticated access to specific paths (e.g., health checks)
    excludePaths:
      - /healthz
      - /readyz

# ============================================================================
# KEDA Configuration (Advanced Autoscaling)
# KEDA enables scale-to-zero, custom metrics scaling, and cron-based scaling
# See: https://keda.sh/docs/
#
# IMPORTANT: If KEDA is already installed in your cluster (e.g., via
# `helm install keda kedacore/keda`), you MUST keep this disabled to avoid
# CRD ownership conflicts. The chart will detect existing KEDA installations
# and fail with a helpful message if you try to enable this subchart.
# ============================================================================

keda:
  # -- Enable KEDA (Kubernetes Event-driven Autoscaler)
  # Required for AgentRuntime autoscaling with type: keda
  # Set to false if KEDA is already installed in your cluster
  enabled: false
  # -- KEDA operator configuration
  # See: https://github.com/kedacore/charts/tree/main/keda
  operator:
    # -- Watch all namespaces (recommended for cluster-wide use)
    watchNamespace: ""
  # -- Prometheus metrics adapter
  prometheus:
    # -- Prometheus server address for KEDA triggers
    # This is the default when using the Prometheus subchart
    serverAddress: "http://omnia-prometheus-server.omnia-system.svc.cluster.local"

# ============================================================================
# Demo Mode (Separate Chart)
# Demo agents are now deployed via the omnia-demos chart.
# See: charts/omnia-demos/
# ============================================================================
