# Default values for omnia.
# This is a YAML-formatted file.
#
# =============================================================================
# Table of Contents
# =============================================================================
#
# Core Configuration:
#   - Basic Settings (replicaCount, image, serviceAccount)  ~Line 30
#   - License Configuration                                  ~Line 64
#   - API Configuration                                      ~Line 80
#
# Enterprise Features (requires license):
#   - Enterprise Settings (enterprise.enabled, arena)        ~Line 10
#
# Dashboard:
#   - Dashboard Configuration                                ~Line 180
#
# Multi-Tenancy:
#   - Workspace Configuration                                ~Line 430
#   - Workspace Content Storage                              ~Line 455
#
# Storage:
#   - NFS Storage Configuration                              ~Line 480
#   - VS Code Server (dev only)                              ~Line 550
#
# Observability:
#   - Tracing Configuration                                  ~Line 600
#   - Prometheus, Grafana, Loki, Tempo, Alloy               ~Line 615
#
# Networking:
#   - Gateway API Configuration                              ~Line 965
#   - Istio Service Mesh Integration                         ~Line 950
#
# Advanced:
#   - KEDA Autoscaling                                       ~Line 1010
#   - Redis                                                  ~Line 1050
#   - Authentication                                         ~Line 1075
#
# =============================================================================

# -- Number of operator replicas
replicaCount: 1

# -- Enable development mode with full-featured license. DO NOT USE IN PRODUCTION.
devMode: false

# ============================================================================
# Enterprise Features (requires license)
# ============================================================================
enterprise:
  # -- Enable enterprise features (Arena, licensing)
  enabled: false

  # Arena controller configuration
  arena:
    controller:
      # -- Number of arena controller replicas
      replicaCount: 1
      image:
        # -- Arena controller image repository
        repository: ghcr.io/altairalabs/omnia-arena-controller
        # -- Arena controller image tag (defaults to Chart appVersion)
        tag: ""
        # -- Arena controller image pull policy
        pullPolicy: IfNotPresent
      # -- Resource limits for arena controller
      resources: {}

    # Worker configuration for ArenaJob execution
    worker:
      image:
        # -- Worker image repository
        repository: ghcr.io/altairalabs/omnia-arena-worker
        # -- Worker image tag (defaults to Chart appVersion)
        tag: ""
        # -- Worker image pull policy
        pullPolicy: IfNotPresent

      # -- Default resource limits for worker pods
      resources:
        limits:
          cpu: 1000m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi

    # Work queue configuration for job distribution
    queue:
      # -- Queue backend type: memory (dev) or redis (prod)
      type: memory

      # Redis configuration (when type: redis)
      redis:
        # -- Redis host (auto-configured when redis.enabled=true)
        host: ""
        # -- Redis port
        port: 6379

  # Community templates source (deployed automatically when enabled)
  communityTemplates:
    # -- Enable community templates source
    enabled: true
    # -- Name of the ArenaTemplateSource resource
    name: community-templates
    # -- Namespace to deploy the ArenaTemplateSource
    namespace: dev-agents
    # -- Git repository URL for community templates
    url: https://github.com/AltairaLabs/promptkit-templates.git
    # -- Branch to track
    branch: main
    # -- Path within fetched content where templates are located (empty = root)
    templatesPath: ""
    # -- Sync interval for fetching updates
    syncInterval: 1h
    # -- Fetch timeout
    timeout: 2m

  # PromptKit LSP server for YAML validation
  promptkitLsp:
    # -- Enable PromptKit LSP server
    enabled: false
    # -- Number of PromptKit LSP replicas
    replicaCount: 2
    image:
      # -- PromptKit LSP image repository
      repository: ghcr.io/altairalabs/omnia-promptkit-lsp
      # -- PromptKit LSP image tag (defaults to Chart appVersion)
      tag: ""
      # -- PromptKit LSP image pull policy
      pullPolicy: IfNotPresent
    service:
      # -- PromptKit LSP service port
      port: 8080
    resources: {}

# License configuration
license:
  # -- License key (JWT). If set, creates the arena-license Secret.
  # Generate at https://omnia.altairalabs.ai/trial or via license server.
  key: ""
  # -- Use an existing Secret instead of creating one.
  # The secret must have a 'license' key containing the JWT.
  existingSecret: ""

image:
  # -- Image repository
  repository: ghcr.io/altairalabs/omnia
  # -- Image pull policy
  pullPolicy: IfNotPresent
  # -- Image tag (defaults to Chart appVersion)
  tag: ""

# -- Image pull secrets
imagePullSecrets: []

# -- Override the name of the chart
nameOverride: ""

# -- Override the full name of the chart
fullnameOverride: ""

serviceAccount:
  # -- Create a service account
  create: true
  # -- Annotations to add to the service account
  annotations: {}
  # -- The name of the service account (generated if not set)
  name: ""

# -- Pod annotations
podAnnotations: {}

# -- Pod security context
podSecurityContext:
  runAsNonRoot: true
  seccompProfile:
    type: RuntimeDefault

# -- Container security context
securityContext:
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL

# -- Resource limits and requests
resources:
  limits:
    cpu: 500m
    memory: 256Mi
  requests:
    cpu: 10m
    memory: 128Mi

# Pod Disruption Budget for HA deployments
podDisruptionBudget:
  # -- Enable PDB for operator
  enabled: false
  # -- Minimum available pods (mutually exclusive with maxUnavailable)
  minAvailable: 1
  # -- Maximum unavailable pods (mutually exclusive with minAvailable)
  # maxUnavailable: 1

# -- Node selector
nodeSelector: {}

# -- Tolerations
tolerations: []

# -- Affinity rules
affinity: {}

# Leader election configuration
leaderElection:
  # -- Enable leader election for HA
  enabled: true

# Health probes configuration
probes:
  # -- Port for health probes
  port: 8081
  liveness:
    # -- Initial delay for liveness probe
    initialDelaySeconds: 15
    # -- Period for liveness probe
    periodSeconds: 20
  readiness:
    # -- Initial delay for readiness probe
    initialDelaySeconds: 5
    # -- Period for readiness probe
    periodSeconds: 10

# Metrics configuration
metrics:
  # -- Enable metrics endpoint
  enabled: false
  # -- Port for metrics
  port: 8443
  # -- Enable secure metrics (HTTPS)
  secure: true

# Webhook configuration
webhook:
  # -- Enable webhooks
  enabled: false
  # -- Port for webhooks
  port: 9443

# RBAC configuration
rbac:
  # -- Create RBAC resources
  create: true

# CRD configuration
crds:
  # -- Install CRDs with the chart
  install: true

# ============================================================================
# Dashboard Configuration
# Web UI for managing Omnia agents and resources
# ============================================================================

dashboard:
  # -- Enable dashboard deployment
  enabled: true

  # -- Hide enterprise features completely instead of showing upgrade prompts
  # When false (default): Shows enterprise features with upgrade prompts when enterprise.enabled=false
  # When true: Hides enterprise features entirely from the UI
  hideEnterprise: false

  image:
    # -- Dashboard image repository
    repository: ghcr.io/altairalabs/omnia-dashboard
    # -- Dashboard image pull policy
    pullPolicy: IfNotPresent
    # -- Dashboard image tag (defaults to Chart appVersion)
    tag: ""

  # -- Number of dashboard replicas
  replicaCount: 1

  # -- Dashboard service configuration
  service:
    # -- Service type (ClusterIP, NodePort, LoadBalancer)
    type: ClusterIP
    # -- Service port
    port: 3000

  # -- Dashboard ingress configuration
  ingress:
    # -- Enable ingress
    enabled: false
    # -- Ingress class name
    className: ""
    # -- Ingress annotations
    annotations: {}
    # -- Ingress host
    host: dashboard.example.com
    # -- TLS configuration
    tls: []
    #  - secretName: dashboard-tls
    #    hosts:
    #      - dashboard.example.com

  # -- Dashboard resource limits and requests
  resources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi

  # Pod Disruption Budget for HA deployments
  podDisruptionBudget:
    # -- Enable PDB for dashboard
    enabled: false
    # -- Minimum available pods (mutually exclusive with maxUnavailable)
    minAvailable: 1
    # -- Maximum unavailable pods (mutually exclusive with minAvailable)
    # maxUnavailable: 1

  # -- Dashboard pod annotations
  podAnnotations: {}

  # -- Dashboard node selector
  nodeSelector: {}

  # -- Dashboard tolerations
  tolerations: []

  # -- Dashboard affinity rules
  affinity: {}

  # -- Dashboard startup probe (optional, for slow-starting containers)
  startupProbe: {}
  # Example:
  #   httpGet:
  #     path: /api/health
  #     port: http
  #   initialDelaySeconds: 0
  #   periodSeconds: 10
  #   timeoutSeconds: 5
  #   failureThreshold: 30

  # -- Dashboard liveness probe configuration
  livenessProbe:
    # -- Initial delay before starting liveness probes
    initialDelaySeconds: 10
    # -- How often to perform the probe
    periodSeconds: 30
    # -- Timeout for each probe
    timeoutSeconds: 5
    # -- Number of failures before marking unhealthy
    failureThreshold: 3

  # -- Dashboard readiness probe configuration
  readinessProbe:
    # -- Initial delay before starting readiness probes
    initialDelaySeconds: 5
    # -- How often to perform the probe
    periodSeconds: 10
    # -- Timeout for each probe
    timeoutSeconds: 5
    # -- Number of failures before marking not ready
    failureThreshold: 3

  # -- Authentication configuration
  auth:
    # -- Authentication mode: anonymous, proxy, oauth, or builtin
    mode: anonymous
    # -- Session secret (required for non-anonymous modes)
    # Generate with: openssl rand -base64 32
    sessionSecret: ""
    # -- Use existing secret for session secret
    # Secret must contain OMNIA_SESSION_SECRET key
    existingSessionSecret: ""
    # -- Session cookie name
    cookieName: omnia_session
    # -- Session TTL in seconds (default: 24 hours)
    ttl: 86400
    # -- Default role for anonymous users
    anonymousRole: viewer
    # -- Role mapping from groups
    roleMapping:
      # -- Groups that get admin role
      adminGroups: []
      # -- Groups that get editor role
      editorGroups: []

  # -- Proxy authentication settings (when auth.mode=proxy)
  proxy:
    # -- Header containing username
    headerUser: X-Forwarded-User
    # -- Header containing email
    headerEmail: X-Forwarded-Email
    # -- Header containing groups
    headerGroups: X-Forwarded-Groups
    # -- Header containing display name
    headerDisplayName: X-Forwarded-Preferred-Username
    # -- Auto-create users on first login
    autoSignup: true

  # -- OAuth settings (when auth.mode=oauth)
  oauth:
    # -- OAuth provider: generic, google, github, azure, okta
    provider: generic
    # -- OAuth client ID
    clientId: ""
    # -- OAuth client secret
    clientSecret: ""
    # -- Use existing secret for OAuth credentials
    # Secret must contain OMNIA_OAUTH_CLIENT_ID and OMNIA_OAUTH_CLIENT_SECRET keys
    existingSecret: ""
    # -- OIDC issuer URL (required for generic provider)
    issuerUrl: ""
    # -- OAuth scopes (comma-separated)
    scopes: "openid,profile,email"
    # -- Azure AD tenant ID (for azure provider)
    azureTenantId: ""
    # -- Okta domain (for okta provider)
    oktaDomain: ""
    # -- Claim mapping
    claims:
      username: preferred_username
      email: email
      displayName: name
      groups: groups

  # -- Builtin authentication settings (when auth.mode=builtin)
  builtin:
    # -- Storage backend: sqlite or postgresql
    storeType: sqlite
    # -- SQLite database path (for sqlite store)
    sqlitePath: /data/omnia-users.db
    # -- PostgreSQL connection URL (for postgresql store)
    # Use existingSecret for production
    postgresUrl: ""
    # -- Use existing secret for PostgreSQL URL
    # Secret must contain OMNIA_BUILTIN_POSTGRES_URL key
    existingPostgresSecret: ""
    # -- Allow public signup
    allowSignup: false
    # -- Require email verification
    verifyEmail: false
    # -- Minimum password length
    minPasswordLength: 8
    # -- Max failed login attempts before lockout
    maxFailedAttempts: 5
    # -- Lockout duration in seconds
    lockoutDuration: 900
    # -- Initial admin user (created on first run)
    admin:
      # -- Admin username
      username: admin
      # -- Admin email
      email: admin@example.com
      # -- Admin password (use existingSecret for production)
      password: ""
      # -- Use existing secret for admin password
      # Secret must contain OMNIA_BUILTIN_ADMIN_PASSWORD key
      existingSecret: ""

  # -- API keys configuration
  apiKeys:
    # -- Enable API key authentication
    enabled: true
    # -- Maximum keys per user
    maxPerUser: 10
    # -- Default expiration in days (0 = never)
    defaultExpiration: 90

  # -- Operator API URL (for dashboard to communicate with operator)
  # Defaults to in-cluster service URL
  operatorApiUrl: ""

  # -- Enable read-only mode (disables mutations)
  readOnlyMode: false

  # -- Custom read-only message
  readOnlyMessage: ""

  # -- Grafana integration (direct browser access)
  grafana:
    # -- Grafana URL for embedded metrics (must be browser-accessible)
    # For local dev: http://localhost:3001
    # For production: use ingress URL
    url: ""
    # -- Grafana subpath (must match Grafana's serve_from_sub_path setting)
    path: /grafana/
    # -- Grafana organization ID
    orgId: 1

  # -- Prometheus integration (for cost metrics)
  prometheus:
    # -- Prometheus URL for cost queries
    # Defaults to in-cluster Prometheus service when prometheus.enabled=true
    url: ""

  # -- WebSocket proxy configuration (for agent console connections)
  wsProxy:
    # -- WebSocket proxy URL for browser connections
    # For local dev: ws://localhost:3002
    # For production with gateway: leave empty (uses gateway routing)
    url: ""

  # -- Persistent volume for builtin auth SQLite database
  persistence:
    # -- Enable persistence (required for builtin auth with SQLite)
    enabled: false
    # -- Storage class
    storageClass: ""
    # -- Access modes (ReadWriteMany required for job mounting)
    # For development with local-path, override to: ["ReadWriteOnce"]
    accessModes:
      - ReadWriteMany
    # -- Volume size
    size: 1Gi
    # -- Existing PVC to use
    existingClaim: ""

# ============================================================================
# Workspace Multi-Tenancy Configuration
# Multi-tenant workspace isolation with namespace, RBAC, and quotas
# ============================================================================

workspaces:
  # -- Enable Workspace controller for multi-tenant isolation
  enabled: true

  # Default storage settings for workspace PVCs
  # These are used when Workspace.spec.storage doesn't specify values
  storage:
    # -- Default storage class for workspace content PVCs
    # For production: Use NFS-backed class (efs-sc, azurefile, nfs-client)
    # For development: Leave empty to use cluster default
    storageClass: ""
    # -- Default volume size for workspace content
    size: 10Gi
    # -- Default access modes (ReadWriteMany for jobs to mount same PVC)
    # For development with local-path, override to: ["ReadWriteOnce"]
    accessModes:
      - ReadWriteMany

# ============================================================================
# Workspace Content Storage Configuration
# Dashboard mount for browsing all workspace content
# ============================================================================

workspaceContent:
  # -- Enable workspace content storage for dashboard
  # This creates a root PVC that the dashboard mounts to browse all workspace content
  # NOTE: Only effective when enterprise.enabled=true (automatically gated in templates)
  # Requires RWX storage class (NFS or cloud-native like EFS, Azure Files, Filestore)
  enabled: true

  # -- Mount path in dashboard pod
  mountPath: /workspace-content

  persistence:
    # -- Storage class for dashboard's workspace content PVC
    # For production: Use your cloud's RWX storage class (efs-sc, azurefile-csi-nfs, filestore-sc)
    # For development: Leave empty and enable nfs.csiDriver to use internal NFS
    storageClass: ""
    # -- Access modes for dashboard content PVC (ReadWriteMany for shared access)
    accessModes:
      - ReadWriteMany
    # -- Volume size for dashboard content view
    size: 50Gi
    # -- Use existing PVC instead of creating one
    existingClaim: ""

# ============================================================================
# NFS Storage Configuration
# Shared storage backend for workspace content across namespaces
# ============================================================================

nfs:
  # Internal NFS server for development environments
  server:
    # -- Deploy internal NFS server (dev only, not for production)
    enabled: false
    # -- Use internal NFS server (when enabled, auto-configures CSI driver)
    internal: true
    # -- Storage class for NFS server's backing storage (only for internal server)
    storageClass: ""
    # -- Size of NFS server's backing storage
    size: 50Gi
    image:
      # -- NFS server image repository (erichough/nfs-server is compatible with most K8s distributions)
      repository: erichough/nfs-server
      # -- NFS server image tag
      tag: "2.2.1"
      # -- Image pull policy
      pullPolicy: IfNotPresent
    # -- Resource limits for NFS server pod
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 50m
        memory: 64Mi

  # External NFS server configuration (for production with self-managed NFS)
  external:
    # -- External NFS server hostname or IP
    server: ""
    # -- Export path on NFS server
    path: /exports/omnia

  # NFS CSI driver configuration (csi-driver-nfs subchart)
  # The CSI driver mounts NFS from within pods, so DNS resolution works correctly
  csiDriver:
    # -- Enable NFS CSI driver for dynamic PVC provisioning
    # For local dev: Set to true along with nfs.server.enabled
    # For production: Use cloud-native storage classes instead (EFS, Azure Files, Filestore)
    enabled: false
    # StorageClass configuration
    storageClass:
      # -- Storage class name for NFS-backed PVCs
      name: omnia-nfs
      # -- Reclaim policy for provisioned PVs
      reclaimPolicy: Delete
      # -- Subdirectory for PVC data (optional, creates per-PVC subdirs automatically)
      subDir: ""

# NFS CSI driver subchart values (when nfs.csiDriver.enabled)
# See: https://github.com/kubernetes-csi/csi-driver-nfs
nfscsi:
  # Controller configuration
  controller:
    # -- Number of controller replicas
    replicas: 1
  # Feature gates
  feature:
    # -- Enable fsGroupPolicy for proper file permissions
    enableFSGroupPolicy: true
  # External snapshotter sidecar (disabled - VolumeSnapshot CRDs not installed)
  externalSnapshotter:
    enabled: false

# ============================================================================
# VS Code Server (Development Only)
# Web-based VS Code for browsing/editing workspace content
# ============================================================================

vscodeServer:
  # -- Enable VS Code server (only works when devMode is also true)
  enabled: false
  image:
    # -- VS Code server image repository
    repository: codercom/code-server
    # -- VS Code server image tag
    tag: "4.96.4"
    # -- Image pull policy
    pullPolicy: IfNotPresent
  resources:
    limits:
      cpu: "1"
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 256Mi

# Facade container configuration (used by AgentRuntime)
facade:
  image:
    # -- Facade image repository
    repository: ghcr.io/altairalabs/omnia-facade
    # -- Facade image tag (defaults to Chart appVersion)
    tag: ""
    # -- Facade image pull policy
    pullPolicy: IfNotPresent

# Framework container configuration (used by AgentRuntime)
# This aligns with the CRD's spec.framework field
framework:
  image:
    # -- Framework (runtime) image repository
    repository: ghcr.io/altairalabs/omnia-runtime
    # -- Framework image tag (defaults to Chart appVersion)
    tag: ""
    # -- Framework image pull policy
    pullPolicy: IfNotPresent

# Observability configuration (for Omnia-specific templates)
observability:
  # -- Enable Omnia dashboards/datasources
  enabled: true

# ============================================================================
# Tracing Configuration
# Distributed tracing for agent runtime containers
# ============================================================================

tracing:
  # -- Enable distributed tracing for agent runtime containers
  enabled: false
  # -- OTLP gRPC endpoint for traces
  # When tempo.enabled=true, defaults to the in-cluster Tempo service
  # Example: omnia-tempo.omnia-system.svc.cluster.local:4317
  endpoint: ""

# ============================================================================
# Subchart configurations (optional observability stack)
# Set enabled: true to deploy each component
# ============================================================================

# Prometheus configuration (metrics)
# See: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus
prometheus:
  # -- Enable Prometheus
  enabled: false
  server:
    # -- Disable persistent storage for dev/test
    persistentVolume:
      enabled: false
    # Configure Prometheus to serve from sub-path
    prefixURL: /prometheus
    baseURL: /prometheus
    # Enable remote write receiver for Tempo metrics-generator
    extraFlags:
      - web.enable-remote-write-receiver
  # -- Disable alertmanager
  alertmanager:
    enabled: false
  # -- Disable pushgateway
  prometheus-pushgateway:
    enabled: false
  # -- Disable node-exporter
  prometheus-node-exporter:
    enabled: false
  # -- Disable kube-state-metrics (usually already present)
  kube-state-metrics:
    enabled: false
  # -- Extra scrape configs for Omnia agent metrics
  extraScrapeConfigs: |
    # Scrape Omnia facade containers (WebSocket connections, media uploads)
    # Uses prometheus.io annotations which point to facade port 8080
    - job_name: 'omnia-agents-facade'
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        # Only scrape pods with omnia agent label
        - source_labels: [__meta_kubernetes_pod_label_omnia_altairalabs_ai_component]
          action: keep
          regex: agent
        # Use prometheus.io annotations (works for both Istio and non-Istio pods)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        # Add useful labels
        - source_labels: [__meta_kubernetes_namespace]
          target_label: namespace
        - source_labels: [__meta_kubernetes_pod_name]
          target_label: pod
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_instance]
          target_label: agent
        - target_label: container
          replacement: facade
    # Scrape Omnia runtime containers (LLM metrics: tokens, cost, latency)
    # Runtime exposes metrics on port 9001 from PromptKit EventBus
    - job_name: 'omnia-agents-runtime'
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        # Only scrape pods with omnia agent label
        - source_labels: [__meta_kubernetes_pod_label_omnia_altairalabs_ai_component]
          action: keep
          regex: agent
        # Target runtime container's health port (9001) for LLM metrics
        - source_labels: [__address__]
          action: replace
          regex: ([^:]+)(?::\d+)?
          replacement: $1:9001
          target_label: __address__
        # Add useful labels
        - source_labels: [__meta_kubernetes_namespace]
          target_label: namespace
        - source_labels: [__meta_kubernetes_pod_name]
          target_label: pod
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_instance]
          target_label: agent
        - target_label: container
          replacement: runtime

# Grafana configuration (dashboards)
# See: https://github.com/grafana/helm-charts/tree/main/charts/grafana
grafana:
  # -- Enable Grafana
  enabled: false
  # -- Admin password (change in production!)
  adminPassword: admin
  # Configure Grafana to serve from sub-path
  grafana.ini:
    server:
      root_url: "%(protocol)s://%(domain)s:%(http_port)s/grafana/"
      serve_from_sub_path: true
    # Enable anonymous access for iframe embedding
    auth.anonymous:
      enabled: true
      org_role: Viewer
    # Allow embedding in iframes
    security:
      allow_embedding: true
  # -- Grafana plugins to install
  # Redis plugin provides Redis datasource for monitoring and visualization
  plugins:
    - redis-datasource
  # Sidecar configuration for auto-loading dashboards/datasources
  sidecar:
    dashboards:
      # -- Enable dashboard sidecar
      enabled: true
      # -- Label to identify dashboard ConfigMaps
      label: grafana_dashboard
      # -- Search all namespaces for dashboards
      searchNamespace: ALL
    datasources:
      # -- Enable datasource sidecar
      enabled: true
      # -- Label to identify datasource ConfigMaps
      label: grafana_datasource
      # -- Search all namespaces for datasources
      searchNamespace: ALL
  # -- Grafana service type
  service:
    type: ClusterIP

# Loki configuration (logs)
# See: https://github.com/grafana/helm-charts/tree/main/charts/loki
loki:
  # -- Enable Loki
  enabled: false
  # -- Deploy in single binary mode
  deploymentMode: SingleBinary
  loki:
    # -- Disable auth for simplicity
    auth_enabled: false
    # -- Use test schema for quick deployment
    useTestSchema: true
    # -- Use filesystem storage
    storage:
      type: filesystem
    # -- Common config for single-binary mode
    commonConfig:
      replication_factor: 1
    # -- Enable log volume for Grafana Explore
    limits_config:
      volume_enabled: true
  singleBinary:
    # -- Single replica for dev/test
    replicas: 1
    persistence:
      # -- Enable persistence for dev/test
      enabled: true
      size: 10Gi
  # -- Disable distributed components
  backend:
    replicas: 0
  read:
    replicas: 0
  write:
    replicas: 0
  # -- Disable gateway
  gateway:
    enabled: false
  # -- Disable caches for simpler deployment
  chunksCache:
    enabled: false
  resultsCache:
    enabled: false
  # -- Disable ruler to avoid "no space left on device" errors on Docker Desktop
  # The ruler tries to mkdir /var/loki/rules which fails with local-path provisioner
  # See: https://github.com/grafana/helm-charts/issues/577
  ruler:
    enabled: false

# Alloy configuration (unified telemetry collector)
# See: https://github.com/grafana/helm-charts/tree/main/charts/alloy
# Alloy replaces Promtail and can collect logs, metrics, and traces
alloy:
  # -- Enable Alloy telemetry collector
  enabled: false
  alloy:
    # Alloy configuration using River syntax
    configMap:
      content: |
        // Discover Kubernetes pods
        discovery.kubernetes "pods" {
          role = "pod"
        }

        // Relabel discovered pods for log collection
        discovery.relabel "pods" {
          targets = discovery.kubernetes.pods.targets

          // Keep only running pods
          rule {
            source_labels = ["__meta_kubernetes_pod_phase"]
            regex         = "Pending|Succeeded|Failed|Unknown"
            action        = "drop"
          }

          // Set namespace label
          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            target_label  = "namespace"
          }

          // Set pod label
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            target_label  = "pod"
          }

          // Set container label
          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            target_label  = "container"
          }

          // Set app label from pod label
          rule {
            source_labels = ["__meta_kubernetes_pod_label_app"]
            target_label  = "app"
          }

          // Set app.kubernetes.io/name label
          rule {
            source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
            target_label  = "app_name"
          }

          // Build log file path
          rule {
            source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
            separator     = "/"
            target_label  = "__path__"
            replacement   = "/var/log/pods/*$1/*.log"
          }
        }

        // Collect logs from discovered pods
        loki.source.kubernetes "pods" {
          targets    = discovery.relabel.pods.output
          forward_to = [loki.process.pods.receiver]
        }

        // Process logs (parse JSON, extract level, filter noise)
        loki.process "pods" {
          forward_to = [loki.write.default.receiver]

          // Parse CRI format
          stage.cri {}

          // Drop noisy health check and metrics logs
          stage.drop {
            expression  = ".*(GET|POST) /(health|healthz|ready|readyz|metrics|livez).*"
            drop_counter_reason = "health_metrics_filter"
          }

          // Try to parse JSON logs and extract timestamp
          stage.json {
            expressions = {
              level     = "level",
              msg       = "msg",
              caller    = "caller",
              timestamp = "time",
              ts        = "ts",
            }
          }

          // Extract timestamp from JSON 'time' field (common in structured logs)
          stage.timestamp {
            source = "timestamp"
            format = "RFC3339"
            fallback_formats = ["RFC3339Nano", "Unix", "UnixMs", "UnixUs", "UnixNs"]
          }

          // Fallback: try 'ts' field (used by zap and other loggers)
          stage.timestamp {
            source = "ts"
            format = "RFC3339"
            fallback_formats = ["RFC3339Nano", "Unix", "UnixMs", "UnixUs", "UnixNs"]
          }

          // Add level as label if extracted
          stage.labels {
            values = {
              level = "",
            }
          }

          // Store extracted fields as structured metadata (queryable but not indexed)
          // Requires Loki 3.0+ with structured metadata enabled
          stage.structured_metadata {
            values = {
              msg    = "",
              caller = "",
            }
          }

          // Output a clean log line (just the message if available)
          stage.output {
            source = "msg"
          }
        }

        // Write logs to Loki
        loki.write "default" {
          endpoint {
            url = "http://omnia-loki:3100/loki/api/v1/push"
          }
        }

# Tempo configuration (traces)
# See: https://github.com/grafana/helm-charts/tree/main/charts/tempo
tempo:
  # -- Enable Tempo
  enabled: false
  tempo:
    # -- Disable telemetry reporting
    reportingEnabled: false
    # Enable metrics generator for service graphs
    metricsGenerator:
      enabled: true
      remoteWriteUrl: "http://omnia-prometheus-server:80/prometheus/api/v1/write"
  # Disable Jaeger query UI (we use Grafana instead)
  tempoQuery:
    enabled: false
  # -- Enable persistence (required for Tempo storage)
  persistence:
    enabled: true
    size: 5Gi

# ============================================================================
# Istio Service Mesh Integration
# Istio should be installed separately before deploying Omnia
# See: https://istio.io/latest/docs/setup/install/helm/
# ============================================================================

istio:
  # -- Enable Istio integration (Telemetry resources for tracing/logging)
  # Requires Istio to be installed separately in istio-system namespace
  enabled: false
  # -- Tempo service for trace export (adjust if using different release name)
  tempoService: omnia-tempo.omnia-system.svc.cluster.local
  # -- Tempo OTLP port
  tempoPort: 4317

# ============================================================================
# Gateway API Configuration
# Kubernetes Gateway API is required for agent ingress
# ============================================================================

# External gateway for agent traffic (WebSocket connections)
gateway:
  # -- Enable external Gateway for agent ingress (requires Istio or Gateway controller)
  enabled: false
  # -- Gateway name suffix
  name: agents
  # -- Gateway class name (istio when using Istio, or external controller)
  className: istio
  # -- Gateway listener configuration
  listeners:
    # -- HTTP listener for agent WebSocket connections
    http:
      port: 80
      protocol: HTTP
    # -- HTTPS listener (requires TLS secret)
    https:
      enabled: false
      port: 443
      protocol: HTTPS
      # -- TLS secret name (must exist in the namespace)
      tlsSecretName: ""

# Internal gateway for observability tools (Grafana, Prometheus, etc.)
internalGateway:
  # -- Enable internal Gateway for observability tools (requires Istio or Gateway controller)
  enabled: false
  # -- Gateway name suffix
  name: internal
  # -- Gateway class name
  className: istio
  # -- Gateway listener port
  port: 8080
  # -- Expose Grafana (main observability UI - includes Prometheus, Loki, Tempo datasources)
  grafana:
    enabled: true
    path: /grafana
  # -- Expose Prometheus UI (optional - metrics also available in Grafana)
  prometheus:
    enabled: true
    path: /prometheus

# ============================================================================
# Authentication Configuration
# JWT-based authentication using Istio RequestAuthentication
# ============================================================================

authentication:
  # -- Enable JWT authentication for agent endpoints
  enabled: false

  # -- OIDC/JWT provider configuration
  jwt:
    # -- JWT issuer URL (e.g., https://auth.example.com, https://accounts.google.com)
    issuer: ""
    # -- JWKS URI for validating JWT signatures
    # If empty, defaults to {issuer}/.well-known/jwks.json
    jwksUri: ""
    # -- JWT audiences to accept (optional, validates 'aud' claim)
    audiences: []
    # -- Forward the original token to the upstream service
    forwardOriginalToken: true
    # -- Headers to output payload claims (optional)
    # Example: outputClaimToHeaders:
    #   - header: x-user-id
    #     claim: sub
    outputClaimToHeaders: []

  # -- Authorization rules (optional, defaults to requiring valid JWT)
  authorization:
    # -- Require specific claims in the JWT
    # Example: requiredClaims:
    #   - claim: "scope"
    #     values: ["agents:access"]
    requiredClaims: []
    # -- Allow unauthenticated access to specific paths (e.g., health checks)
    excludePaths:
      - /healthz
      - /readyz

# ============================================================================
# KEDA Configuration (Advanced Autoscaling)
# KEDA enables scale-to-zero, custom metrics scaling, and cron-based scaling
# See: https://keda.sh/docs/
#
# IMPORTANT: If KEDA is already installed in your cluster (e.g., via
# `helm install keda kedacore/keda`), you MUST keep this disabled to avoid
# CRD ownership conflicts. The chart will detect existing KEDA installations
# and fail with a helpful message if you try to enable this subchart.
# ============================================================================

keda:
  # -- Enable KEDA (Kubernetes Event-driven Autoscaler)
  # Required for AgentRuntime autoscaling with type: keda
  # Set to false if KEDA is already installed in your cluster
  enabled: false
  # -- KEDA operator configuration
  # See: https://github.com/kedacore/charts/tree/main/keda
  operator:
    # -- Watch all namespaces (recommended for cluster-wide use)
    watchNamespace: ""
  # -- Prometheus metrics adapter
  prometheus:
    # -- Prometheus server address for KEDA triggers
    # This is the default when using the Prometheus subchart
    serverAddress: "http://omnia-prometheus-server.omnia-system.svc.cluster.local"

# ============================================================================
# Redis (Bitnami)
# Optional Redis deployment for Arena queue, sessions, caching, etc.
# See: https://github.com/bitnami/charts/tree/main/bitnami/redis
# ============================================================================

redis:
  # -- Enable Redis subchart (Bitnami Redis)
  # Use this for development or when you want a managed Redis instance
  enabled: false

  # -- Subchart values for Bitnami Redis
  # See: https://github.com/bitnami/charts/tree/main/bitnami/redis
  # Architecture: standalone (1 master) or replication (master + replicas)
  architecture: standalone

  auth:
    # -- Disable auth for development (enable and set password for production)
    enabled: false

  master:
    persistence:
      # -- Disable persistence for development
      enabled: false
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 50m
        memory: 64Mi

  replica:
    # -- Number of replicas (only used when architecture: replication)
    replicaCount: 0

# ============================================================================
# Demo Mode (Separate Chart)
# Demo agents are now deployed via the omnia-demos chart.
# See: charts/omnia-demos/
# ============================================================================
