name: Helm E2E Tests

on:
  push:
    branches:
      - main
    paths:
      - 'charts/**'
      - '.github/workflows/test-helm-e2e.yml'
  pull_request:
    paths:
      - 'charts/**'
      - '.github/workflows/test-helm-e2e.yml'

permissions:
  contents: read
  checks: write

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}
  cancel-in-progress: true

env:
  KIND_CLUSTER: helm-e2e-test
  HELM_VERSION: v3.14.0

jobs:
  helm-e2e:
    name: Helm Chart Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Helm
        uses: azure/setup-helm@v4
        with:
          version: ${{ env.HELM_VERSION }}

      - name: Install kind
        run: |
          curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
          chmod +x ./kind
          sudo mv ./kind /usr/local/bin/kind
          kind version

      - name: Create kind cluster
        run: |
          kind create cluster --name $KIND_CLUSTER --wait 60s
          kubectl cluster-info --context kind-$KIND_CLUSTER

      - name: Install cert-manager
        run: |
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.4/cert-manager.yaml
          echo "Waiting for cert-manager to be ready..."
          kubectl wait --for=condition=Available deployment/cert-manager -n cert-manager --timeout=120s
          kubectl wait --for=condition=Available deployment/cert-manager-webhook -n cert-manager --timeout=120s
          kubectl wait --for=condition=Available deployment/cert-manager-cainjector -n cert-manager --timeout=120s

      - name: Install Gateway API CRDs
        run: |
          kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.2.0/standard-install.yaml

      - name: Add Helm repositories
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo add kedacore https://kedacore.github.io/charts
          helm repo update

      - name: Build chart dependencies
        run: |
          helm dependency build ./charts/omnia

      - name: Test - Basic Helm template validation
        run: |
          echo "=== Testing helm template generation ==="
          helm template omnia ./charts/omnia --debug > /dev/null
          echo "Template generation successful"

      # ============================================================
      # Test Group 1: Basic install, upgrade, uninstall, KEDA
      # ============================================================

      - name: Test - Helm install with KEDA subchart disabled
        run: |
          echo "=== Testing basic install (keda.enabled=false) ==="
          helm install omnia ./charts/omnia \
            --namespace omnia-system \
            --create-namespace \
            --set keda.enabled=false \
            --timeout 2m

          echo "Verifying resources were created..."
          kubectl get deployments -n omnia-system
          kubectl get services -n omnia-system
          kubectl get serviceaccounts -n omnia-system

          # Verify CRDs were installed
          kubectl get crd agentruntimes.omnia.altairalabs.ai
          kubectl get crd providers.omnia.altairalabs.ai

          echo "Basic install successful - all resources created"

      - name: Test - Helm upgrade
        run: |
          echo "=== Testing helm upgrade ==="
          helm upgrade omnia ./charts/omnia \
            --namespace omnia-system \
            --set keda.enabled=false \
            --timeout 2m

          echo "Upgrade successful"

      - name: Test - Uninstall Omnia resources
        run: |
          echo "=== Testing helm uninstall cleans up Omnia resources ==="

          # Record what exists before uninstall
          echo "Resources before uninstall:"
          kubectl get deployments -n omnia-system -o name
          kubectl get services -n omnia-system -o name
          kubectl get crd -o name | grep omnia || true

          # Uninstall
          helm uninstall omnia -n omnia-system
          sleep 5

          # Verify our resources are cleaned up
          echo "Verifying Omnia resources are removed..."

          # Check deployments are gone
          if kubectl get deployment omnia-controller-manager -n omnia-system 2>/dev/null; then
            echo "ERROR: Controller deployment still exists"
            exit 1
          fi
          echo "SUCCESS: Controller deployment removed"

          if kubectl get deployment omnia-dashboard -n omnia-system 2>/dev/null; then
            echo "ERROR: Dashboard deployment still exists"
            exit 1
          fi
          echo "SUCCESS: Dashboard deployment removed"

          # Check CRDs are gone
          if kubectl get crd agentruntimes.omnia.altairalabs.ai 2>/dev/null; then
            echo "ERROR: AgentRuntime CRD still exists"
            exit 1
          fi
          echo "SUCCESS: CRDs removed"

          echo "Uninstall test passed - all Omnia resources cleaned up"

      # Recreate cluster for KEDA tests (faster than cleaning up KEDA's resources)
      - name: Recreate cluster for KEDA tests
        run: |
          kind delete cluster --name $KIND_CLUSTER
          kind create cluster --name $KIND_CLUSTER --wait 60s
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.4/cert-manager.yaml
          kubectl wait --for=condition=Available deployment/cert-manager -n cert-manager --timeout=120s
          kubectl wait --for=condition=Available deployment/cert-manager-webhook -n cert-manager --timeout=120s
          kubectl wait --for=condition=Available deployment/cert-manager-cainjector -n cert-manager --timeout=120s

      - name: Test - KEDA conflict detection
        run: |
          echo "=== Testing KEDA conflict detection ==="

          # Install KEDA separately
          echo "Installing KEDA separately..."
          helm install keda kedacore/keda \
            --namespace keda \
            --create-namespace \
            --wait \
            --timeout 5m

          # Verify KEDA CRD exists
          kubectl get crd scaledobjects.keda.sh

          # Try to install omnia with keda.enabled=true - should FAIL
          echo "Attempting to install omnia with keda.enabled=true (should fail)..."
          set +e
          helm install omnia ./charts/omnia \
            --namespace omnia-system \
            --create-namespace \
            --set keda.enabled=true \
            --timeout 1m 2>&1 | tee /tmp/helm-output.txt
          HELM_EXIT_CODE=${PIPESTATUS[0]}
          set -e

          if [ $HELM_EXIT_CODE -eq 0 ]; then
            echo "ERROR: Installation should have failed but succeeded!"
            exit 1
          fi
          echo "Helm install failed as expected (exit code: $HELM_EXIT_CODE)"

          if grep -q "KEDA is already installed" /tmp/helm-output.txt; then
            echo "Got expected KEDA conflict error message"
          else
            echo "ERROR: Did not get expected KEDA conflict error message"
            cat /tmp/helm-output.txt
            exit 1
          fi

          # Install with keda.enabled=false - should SUCCEED
          echo "Installing omnia with keda.enabled=false (should succeed)..."
          helm install omnia ./charts/omnia \
            --namespace omnia-system \
            --create-namespace \
            --set keda.enabled=false \
            --timeout 2m

          echo "KEDA conflict detection test passed!"

      - name: Test - Verify ScaledObject works with external KEDA
        run: |
          echo "=== Testing ScaledObject with external KEDA ==="

          cat <<EOF | kubectl apply -f -
          apiVersion: keda.sh/v1alpha1
          kind: ScaledObject
          metadata:
            name: test-scaledobject
            namespace: omnia-system
          spec:
            scaleTargetRef:
              name: omnia-controller-manager
            minReplicaCount: 1
            maxReplicaCount: 3
            triggers:
              - type: cpu
                metadata:
                  type: Utilization
                  value: "80"
          EOF

          kubectl get scaledobject test-scaledobject -n omnia-system
          echo "ScaledObject creation successful with external KEDA"

      # ============================================================
      # Test Group 2: Observability stack (fresh cluster)
      # ============================================================

      - name: Recreate cluster for observability tests
        run: |
          kind delete cluster --name $KIND_CLUSTER
          kind create cluster --name $KIND_CLUSTER --wait 60s
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.4/cert-manager.yaml
          kubectl wait --for=condition=Available deployment/cert-manager -n cert-manager --timeout=120s
          kubectl wait --for=condition=Available deployment/cert-manager-webhook -n cert-manager --timeout=120s
          kubectl wait --for=condition=Available deployment/cert-manager-cainjector -n cert-manager --timeout=120s

      - name: Test - Observability stack deployment
        run: |
          echo "=== Testing observability stack (Prometheus, Grafana, Loki) ==="

          helm install omnia ./charts/omnia \
            --namespace omnia-system \
            --create-namespace \
            --set keda.enabled=false \
            --set prometheus.enabled=true \
            --set grafana.enabled=true \
            --set loki.enabled=true \
            --timeout 5m

          echo "Waiting for observability pods to be created..."
          sleep 30

          echo "=== Checking Prometheus ==="
          kubectl get pods -n omnia-system -l "app.kubernetes.io/name=prometheus" || kubectl get pods -n omnia-system | grep prometheus || true
          kubectl get svc -n omnia-system | grep prometheus || true

          echo "=== Checking Grafana ==="
          kubectl get pods -n omnia-system -l "app.kubernetes.io/name=grafana" || kubectl get pods -n omnia-system | grep grafana || true
          kubectl get svc -n omnia-system | grep grafana || true

          echo "=== Checking Loki ==="
          kubectl get pods -n omnia-system -l "app.kubernetes.io/name=loki" || kubectl get pods -n omnia-system | grep loki || true
          kubectl get svc -n omnia-system | grep loki || true

          kubectl get svc -n omnia-system

          # Verify Loki is not crashing
          echo "=== Verifying Loki is not crashing ==="
          sleep 30
          if kubectl get pods -n omnia-system | grep -i loki | grep -q "CrashLoopBackOff"; then
            echo "ERROR: Loki is in CrashLoopBackOff!"
            kubectl logs -n omnia-system -l "app.kubernetes.io/name=loki" --tail=50 || true
            exit 1
          fi

          echo "Observability stack deployment successful!"

      # ============================================================
      # Test Group 3: Tracing (fresh cluster)
      # ============================================================

      - name: Recreate cluster for tracing tests
        run: |
          kind delete cluster --name $KIND_CLUSTER
          kind create cluster --name $KIND_CLUSTER --wait 60s
          kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.4/cert-manager.yaml
          kubectl wait --for=condition=Available deployment/cert-manager -n cert-manager --timeout=120s
          kubectl wait --for=condition=Available deployment/cert-manager-webhook -n cert-manager --timeout=120s
          kubectl wait --for=condition=Available deployment/cert-manager-cainjector -n cert-manager --timeout=120s

      - name: Test - Prometheus agent scraping configuration
        run: |
          echo "=== Testing Prometheus agent metrics scraping configuration ==="

          # Install with Prometheus and tracing enabled
          helm install omnia ./charts/omnia \
            --namespace omnia-system \
            --create-namespace \
            --set keda.enabled=false \
            --set prometheus.enabled=true \
            --set prometheus.server.persistentVolume.enabled=false \
            --set prometheus.alertmanager.enabled=false \
            --set tracing.enabled=true \
            --set tracing.endpoint=omnia-tempo.omnia-system.svc.cluster.local:4317 \
            --set tempo.enabled=true \
            --set tempo.persistence.enabled=false \
            --set grafana.enabled=false \
            --set loki.enabled=false \
            --timeout 5m

          echo "Waiting for Prometheus to be ready..."
          sleep 30

          # Verify Prometheus server is running
          echo "=== Checking Prometheus server ==="
          kubectl get pods -n omnia-system -l "app.kubernetes.io/name=prometheus" -o wide

          # Check Prometheus ConfigMap for omnia-agents scrape job
          echo "=== Verifying omnia-agents scrape job in Prometheus config ==="
          PROM_CONFIG=$(kubectl get configmap -n omnia-system -l "app.kubernetes.io/name=prometheus" -o yaml)

          if echo "$PROM_CONFIG" | grep -q "omnia-agents"; then
            echo "SUCCESS: Found omnia-agents scrape job in Prometheus configuration"
          else
            echo "ERROR: omnia-agents scrape job not found in Prometheus configuration"
            echo "Prometheus ConfigMap contents:"
            echo "$PROM_CONFIG"
            exit 1
          fi

          # Verify the scrape job targets agent pods by label
          if echo "$PROM_CONFIG" | grep -q "omnia_altairalabs_ai_component"; then
            echo "SUCCESS: Scrape job targets pods with omnia.altairalabs.ai/component label"
          else
            echo "ERROR: Scrape job not configured to target agent component label"
            exit 1
          fi

          # Verify the scrape job uses custom Istio-safe annotation for port
          if echo "$PROM_CONFIG" | grep -q "omnia_altairalabs_ai_metrics_port"; then
            echo "SUCCESS: Scrape job uses Istio-safe custom annotation for metrics port"
          else
            echo "ERROR: Scrape job should use omnia.altairalabs.ai/metrics-port annotation"
            exit 1
          fi

          echo "Prometheus agent scraping configuration test passed!"

      - name: Test - Tempo tracing deployment
        run: |
          echo "=== Testing Tempo distributed tracing ==="

          # Verify Tempo is running
          echo "Waiting for Tempo to be ready..."
          kubectl wait --for=condition=Ready pod -l "app.kubernetes.io/name=tempo" \
            -n omnia-system --timeout=120s || {
            echo "Tempo pods:"
            kubectl get pods -n omnia-system -l "app.kubernetes.io/name=tempo" -o wide
            kubectl describe pods -n omnia-system -l "app.kubernetes.io/name=tempo"
            exit 1
          }

          echo "=== Checking Tempo service ==="
          kubectl get svc -n omnia-system | grep tempo

          # Verify Tempo endpoints are accessible
          echo "=== Testing Tempo API accessibility ==="
          kubectl run tempo-test --rm -i --restart=Never \
            --image=curlimages/curl:latest \
            --namespace=omnia-system \
            -- curl -s -o /dev/null -w "%{http_code}" \
               http://omnia-tempo.omnia-system.svc.cluster.local:3200/ready \
            | grep -q "200" && echo "SUCCESS: Tempo is ready" || {
            echo "WARNING: Could not verify Tempo readiness via API"
          }

          echo "Tempo tracing deployment test passed!"

      - name: Test - Tracing environment variables in Helm template
        run: |
          echo "=== Testing tracing configuration in Helm templates ==="

          # Generate template with tracing enabled and check for env vars
          TEMPLATE_OUTPUT=$(helm template omnia ./charts/omnia \
            --set tracing.enabled=true \
            --set tracing.endpoint=tempo.example.com:4317)

          # Verify tracing flags are passed to operator
          if echo "$TEMPLATE_OUTPUT" | grep -q "tracing-enabled=true"; then
            echo "SUCCESS: --tracing-enabled flag is set in operator deployment"
          else
            echo "ERROR: --tracing-enabled flag not found in operator deployment"
            exit 1
          fi

          if echo "$TEMPLATE_OUTPUT" | grep -q "tracing-endpoint=tempo.example.com:4317"; then
            echo "SUCCESS: --tracing-endpoint flag is set correctly"
          else
            echo "ERROR: --tracing-endpoint flag not found or incorrect"
            exit 1
          fi

          echo "Tracing Helm template configuration test passed!"

      # ============================================================
      # Test Group 4: Template-only tests (no cluster needed)
      # ============================================================

      - name: Test - Istio telemetry configuration (template only)
        run: |
          echo "=== Testing Istio telemetry Helm templates ==="

          # Generate template with Istio enabled
          TEMPLATE_OUTPUT=$(helm template omnia ./charts/omnia \
            --set istio.enabled=true \
            --set istio.tempoService=omnia-tempo.omnia-system.svc.cluster.local \
            --set istio.tempoPort=4317)

          # Verify Telemetry resource is created
          if echo "$TEMPLATE_OUTPUT" | grep -q "kind: Telemetry"; then
            echo "SUCCESS: Telemetry resource is created when istio.enabled=true"
          else
            echo "ERROR: Telemetry resource not found in template output"
            exit 1
          fi

          # Verify Telemetry is in istio-system namespace
          if echo "$TEMPLATE_OUTPUT" | grep -A5 "kind: Telemetry" | grep -q "namespace: istio-system"; then
            echo "SUCCESS: Telemetry resource is in istio-system namespace"
          else
            echo "ERROR: Telemetry resource should be in istio-system namespace for mesh-wide config"
            exit 1
          fi

          # Verify OpenTelemetry tracing provider is configured
          if echo "$TEMPLATE_OUTPUT" | grep -q "opentelemetry"; then
            echo "SUCCESS: OpenTelemetry tracing provider is configured"
          else
            echo "ERROR: OpenTelemetry provider not found in Telemetry resource"
            exit 1
          fi

          # Verify access logging is enabled
          if echo "$TEMPLATE_OUTPUT" | grep -q "accessLogging"; then
            echo "SUCCESS: Access logging is configured in Telemetry resource"
          else
            echo "WARNING: Access logging not found (may be optional)"
          fi

          echo "Istio telemetry template configuration test passed!"

      - name: Test - Template with tracing disabled
        run: |
          echo "=== Testing Helm template with tracing disabled ==="

          TEMPLATE_OUTPUT=$(helm template omnia ./charts/omnia \
            --set tracing.enabled=false)

          # Verify tracing flags are NOT present when disabled
          if echo "$TEMPLATE_OUTPUT" | grep -q "tracing-enabled"; then
            echo "ERROR: --tracing-enabled flag should not be present when tracing is disabled"
            exit 1
          fi

          echo "SUCCESS: Tracing flags correctly omitted when disabled"

      - name: Cleanup
        if: always()
        run: |
          kind delete cluster --name $KIND_CLUSTER || true

  helm-lint:
    name: Helm Lint
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Helm
        uses: azure/setup-helm@v4
        with:
          version: ${{ env.HELM_VERSION }}

      - name: Add Helm repositories
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo add kedacore https://kedacore.github.io/charts
          helm repo update

      - name: Build chart dependencies
        run: |
          helm dependency build ./charts/omnia

      - name: Lint chart
        run: |
          helm lint ./charts/omnia

      - name: Validate chart metadata
        run: |
          echo "=== Validating Chart.yaml ==="
          cat ./charts/omnia/Chart.yaml

          # Check required fields
          grep -q "^name:" ./charts/omnia/Chart.yaml
          grep -q "^version:" ./charts/omnia/Chart.yaml
          grep -q "^appVersion:" ./charts/omnia/Chart.yaml

          echo "Chart metadata validation passed"
